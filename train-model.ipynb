{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zlb7jFEQcAZf","executionInfo":{"status":"ok","timestamp":1745002030911,"user_tz":-180,"elapsed":49575,"user":{"displayName":"Nisan","userId":"17072699878303939405"}},"outputId":"b5c30fbd-d2bb-4b36-8eda-a8e1150b4ddf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/DI725/DI725-assignment2\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","%cd /content/drive/MyDrive/DI725/DI725-assignment2/"]},{"cell_type":"markdown","metadata":{"id":"06-VGDHXc6cd"},"source":["We will be working with the [AU-AIR dataset](https://bozcani.github.io/auairdataset), which consists of around 30k annotated low altitute traffic surveillance images with 8 object categories."]},{"cell_type":"code","source":["import os\n","import json\n","import torch\n","import numpy as np\n","from PIL import Image\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import YolosForObjectDetection, YolosConfig, YolosImageProcessor\n","from transformers import Trainer, TrainingArguments\n","from transformers.integrations import TensorBoardCallback, WandbCallback\n","from transformers.trainer_callback import EarlyStoppingCallback\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","from tqdm import tqdm\n","import logging\n","import random\n","import wandb\n","import datetime\n","\n","# Set up logging\n","## no need\n","##logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n","##logger = logging.getLogger(__name__)\n","\n","# Configuration\n","DATA_DIR = \"auair2019data\"\n","IMAGES_DIR = os.path.join(DATA_DIR, \"images\")\n","ANNOTATIONS_FILE = os.path.join(DATA_DIR, \"annotations.json\")\n","OUTPUT_DIR = \"yolos_finetuned\"\n","MODEL_CHECKPOINT = \"hustvl/yolos-small\"\n","BATCH_SIZE = 4  # Smaller batch size to accommodate more images\n","LEARNING_RATE = 3e-5\n","NUM_EPOCHS = 50  # More epochs for large dataset\n","WARMUP_RATIO = 0.05  # Warmup ratio instead of fixed steps for large dataset\n","GRAD_ACCUMULATION_STEPS = 16  # Increased for effective batch size of 64\n","NUM_WORKERS = 8  # For data loading\n","EVAL_STEPS = 2000  # Evaluate less frequently\n","SAVE_STEPS = 2000  # Save less frequently\n","EARLY_STOPPING_PATIENCE = 5  # Stop if no improvement for 5 evaluations\n","\n","# Wandb configuration\n","WANDB_PROJECT = \"object_detection_transformer\"\n","WANDB_NAME = f\"yolos-small-{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n","\n","\n","# Seed everything for reproducibility\n","def seed_everything(seed=42):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","seed_everything()\n","\n","# Initialize wandb\n","print(\"Initializing wandb...\")\n","wandb.init(\n","    project=WANDB_PROJECT,\n","    name=WANDB_NAME,\n","    config={\n","        \"model_checkpoint\": MODEL_CHECKPOINT,\n","        \"batch_size\": BATCH_SIZE,\n","        \"learning_rate\": LEARNING_RATE,\n","        \"epochs\": NUM_EPOCHS,\n","        \"warmup_ratio\": WARMUP_RATIO,\n","        \"grad_accumulation_steps\": GRAD_ACCUMULATION_STEPS,\n","        \"effective_batch_size\": BATCH_SIZE * GRAD_ACCUMULATION_STEPS,\n","        \"early_stopping_patience\": EARLY_STOPPING_PATIENCE,\n","    }\n",")\n","\n","# Load annotations\n","print(\"Loading annotations...\")\n","with open(ANNOTATIONS_FILE, 'r') as f:\n","    data = json.load(f)\n","\n","# Extract category information\n","categories = {idx: cat for idx, cat in enumerate(data['categories'], start=0)}\n","id2label = {k: v for k, v in categories.items()}\n","label2id = {v: k for k, v in id2label.items()}\n","num_labels = len(id2label)\n","\n","print(f\"Found {num_labels} categories: {id2label}\")\n","wandb.config.update({\"num_classes\": num_labels, \"classes\": list(id2label.values())})\n","\n","annotations = data[\"annotations\"]\n","\n","# class distribution\n","category_counts = {class_name: 0 for class_name in id2label.values()}\n","for image_data in annotations:\n","    if \"bbox\" in image_data:\n","        for bbox in image_data[\"bbox\"]:\n","            class_id = bbox[\"class\"]\n","            if class_id in id2label:\n","                category_counts[id2label[class_id]] += 1\n","\n","wandb.log({\"class_distribution\": wandb.Table(\n","    columns=[\"Category\", \"Count\"],\n","    data=[[cat, count] for cat, count in category_counts.items()]\n",")})\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"LjgKlDXXn2-S","executionInfo":{"status":"ok","timestamp":1745002137572,"user_tz":-180,"elapsed":86663,"user":{"displayName":"Nisan","userId":"17072699878303939405"}},"outputId":"6dd1b5b9-ee3b-45b1-de36-e085b171f5c1"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Initializing wandb...\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","        window._wandbApiKey = new Promise((resolve, reject) => {\n","            function loadScript(url) {\n","            return new Promise(function(resolve, reject) {\n","                let newScript = document.createElement(\"script\");\n","                newScript.onerror = reject;\n","                newScript.onload = resolve;\n","                document.body.appendChild(newScript);\n","                newScript.src = url;\n","            });\n","            }\n","            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n","            const iframe = document.createElement('iframe')\n","            iframe.style.cssText = \"width:0;height:0;border:none\"\n","            document.body.appendChild(iframe)\n","            const handshake = new Postmate({\n","                container: iframe,\n","                url: 'https://wandb.ai/authorize'\n","            });\n","            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n","            handshake.then(function(child) {\n","                child.on('authorize', data => {\n","                    clearTimeout(timeout)\n","                    resolve(data)\n","                });\n","            });\n","            })\n","        });\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","wandb: Paste an API key from your profile and hit enter:"]},{"name":"stdout","output_type":"stream","text":[" ··········\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myildizz-nisan\u001b[0m (\u001b[33myildizz-nisan-middle-east-technical-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.19.9"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/drive/MyDrive/DI725/DI725-assignment2/wandb/run-20250418_184851-da3enn2h</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/yildizz-nisan-middle-east-technical-university/object_detection_transformer/runs/da3enn2h' target=\"_blank\">yolos-small-20250418-184758</a></strong> to <a href='https://wandb.ai/yildizz-nisan-middle-east-technical-university/object_detection_transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/yildizz-nisan-middle-east-technical-university/object_detection_transformer' target=\"_blank\">https://wandb.ai/yildizz-nisan-middle-east-technical-university/object_detection_transformer</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/yildizz-nisan-middle-east-technical-university/object_detection_transformer/runs/da3enn2h' target=\"_blank\">https://wandb.ai/yildizz-nisan-middle-east-technical-university/object_detection_transformer/runs/da3enn2h</a>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Loading annotations...\n","Found 8 categories: {0: 'Human', 1: 'Car', 2: 'Truck', 3: 'Van', 4: 'Motorbike', 5: 'Bicycle', 6: 'Bus', 7: 'Trailer'}\n"]}]},{"cell_type":"code","source":["import os\n","from PIL import Image\n","import numpy as np\n","import torch\n","from torch.utils.data import Dataset\n","\n","class CustomObjectDetectionDataset(Dataset):\n","    def __init__(self, annotations, img_dir, processor, transform=None):\n","        self.annotations = annotations\n","        self.img_dir = img_dir\n","        self.processor = processor\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.annotations)\n","\n","    def __getitem__(self, idx):\n","        # Load annotation\n","        ann_data = self.annotations[idx]\n","        img_path = os.path.join(self.img_dir, ann_data[\"image_name\"])\n","\n","        # Load image safely\n","        try:\n","            image = Image.open(img_path).convert(\"RGB\")\n","        except Exception as e:\n","            print(f\"Error loading image {img_path}: {e}\")\n","            image = Image.new('RGB', (640, 640), color='gray')\n","            ann_data[\"bbox\"] = []\n","\n","        image_width, image_height = image.size\n","\n","        # Prepare boxes & labels\n","        boxes = []\n","        labels = []\n","\n","        for bbox in ann_data[\"bbox\"]:\n","            x_min = bbox[\"left\"]\n","            y_min = bbox[\"top\"]\n","            width = bbox[\"width\"]\n","            height = bbox[\"height\"]\n","\n","            x_max = x_min + width\n","            y_max = y_min + height\n","\n","            # Normalize to [0, 1]\n","            x_min = max(0, x_min / image_width)\n","            y_min = max(0, y_min / image_height)\n","            x_max = min(1, x_max / image_width)\n","            y_max = min(1, y_max / image_height)\n","\n","            boxes.append([x_min, y_min, x_max, y_max])\n","            labels.append(bbox[\"class\"])\n","\n","        boxes = np.array(boxes, dtype=np.float32)\n","        labels = np.array(labels, dtype=np.int64)\n","\n","        # Albumentations transform\n","        if self.transform:\n","            transformed = self.transform(image=np.array(image), bboxes=boxes.tolist(), labels=labels.tolist())\n","            image = transformed['image']\n","            boxes = np.array(transformed['bboxes'], dtype=np.float32)\n","            labels = np.array(transformed['labels'], dtype=np.int64)\n","\n","            # Clip out-of-bounds\n","            boxes = np.clip(boxes, 0.0, 1.0)\n","\n","            # Filter out invalid boxes\n","            valid_boxes = []\n","            valid_labels = []\n","            for box, label in zip(boxes, labels):\n","                x_min, y_min, x_max, y_max = box\n","                if x_max > x_min and y_max > y_min:\n","                    valid_boxes.append([x_min, y_min, x_max, y_max])\n","                    valid_labels.append(label)\n","\n","            boxes = np.array(valid_boxes, dtype=np.float32)\n","            labels = np.array(valid_labels, dtype=np.int64)\n","\n","        # Construct annotations for processor\n","        annotations = []\n","        for box, label in zip(boxes, labels):\n","            x_min, y_min, x_max, y_max = box\n","            area = (x_max - x_min) * (y_max - y_min)\n","            annotations.append({\n","                'bbox': [x_min, y_min, x_max, y_max],\n","                'category_id': int(label),\n","                'area': float(area),\n","                'iscrowd': 0\n","            })\n","\n","        # Feed image + annotations into YOLOS processor\n","        encoding = self.processor(\n","            images=image,\n","            annotations={\n","                'image_id': idx,\n","                'annotations': annotations\n","            },\n","            return_tensors=\"pt\"\n","        )\n","\n","        # Remove batch dimension\n","        for k, v in encoding.items():\n","            if isinstance(v, torch.Tensor):\n","                encoding[k] = v.squeeze(0)  # remove batch dim only\n","            else:\n","                encoding[k] = v  # leave non-tensor items as-is\n","\n","        return encoding\n"],"metadata":{"id":"D52a7FnEuJRN","executionInfo":{"status":"ok","timestamp":1745003665049,"user_tz":-180,"elapsed":45,"user":{"displayName":"Nisan","userId":"17072699878303939405"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["\n","# Initialize image processor\n","print(f\"Loading YOLOS image processor from {MODEL_CHECKPOINT}\")\n","processor = YolosImageProcessor.from_pretrained(MODEL_CHECKPOINT)\n","\n","# Define data augmentations\n","\n","train_transform = A.Compose(\n","    [\n","        A.Resize(640, 640),\n","        A.HorizontalFlip(p=0.5),\n","        A.RandomBrightnessContrast(p=0.2),\n","        A.ShiftScaleRotate(\n","            shift_limit=0.05, scale_limit=0.1, rotate_limit=10, border_mode=0, p=0.5\n","        ),\n","    ],\n","    bbox_params=A.BboxParams(\n","        format='yolo',              #\n","        label_fields=['labels'],\n","        min_visibility=0.1,         # Discard boxes too small or barely visible\n","        clip=True,            # clips bboxes to [0, 1]\n","    )\n",")\n","\n","val_transform = A.Compose(\n","    [\n","        A.Resize(640, 640)  #\n","    ],\n","    bbox_params=A.BboxParams(\n","        format='yolo',\n","        label_fields=['labels'],\n","        clip=True,\n","    )\n",")\n","\n","\n","# Log augmentation pipeline to wandb\n","wandb.config.update({\n","    \"augmentations\": str(train_transform),\n","})\n","\n","# Create dataset\n","print(\"Creating datasets...\")\n","full_dataset = CustomObjectDetectionDataset(\n","    annotations,\n","    IMAGES_DIR,\n","    processor\n",")\n","\n","# Split dataset\n","train_size = int(0.8 * len(full_dataset))\n","val_size = len(full_dataset) - train_size\n","train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n","\n","# Apply transforms\n","train_dataset.dataset.transform = train_transform\n","val_dataset.dataset.transform = val_transform\n","\n","print(f\"Train dataset size: {len(train_dataset)}\")\n","print(f\"Validation dataset size: {len(val_dataset)}\")\n","\n","# Log data split info\n","wandb.config.update({\n","    \"train_size\": len(train_dataset),\n","    \"val_size\": len(val_dataset),\n","    \"train_val_ratio\": train_size / val_size if val_size > 0 else \"N/A\",\n","})\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9SiURuxLwU6G","executionInfo":{"status":"ok","timestamp":1745003711085,"user_tz":-180,"elapsed":375,"user":{"displayName":"Nisan","userId":"17072699878303939405"}},"outputId":"d0f2da46-c94f-48a2-cdb0-40f2a619ad6a"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading YOLOS image processor from hustvl/yolos-small\n","Creating datasets...\n","Train dataset size: 26258\n","Validation dataset size: 6565\n"]}]},{"cell_type":"code","source":["\n","# Custom data collator to handle batching and padding\n","def custom_collate_fn(batch):\n","    # Separate inputs and labels\n","    pixel_values = torch.stack([item['pixel_values'] for item in batch])\n","    labels = [item['labels'] for item in batch]\n","    pixel_mask = torch.stack([item['pixel_mask'] for item in batch]) if 'pixel_mask' in batch[0] else None\n","\n","    # Create batch dictionary\n","    batch_dict = {\n","        'pixel_values': pixel_values,\n","        'labels': labels,\n","    }\n","\n","    if pixel_mask is not None:\n","        batch_dict['pixel_mask'] = pixel_mask\n","\n","    return batch_dict\n","\n","# Get model with updated config for your number of classes\n","print(f\"Loading YOLOS model from {MODEL_CHECKPOINT}\")\n","config = YolosConfig.from_pretrained(MODEL_CHECKPOINT, id2label=id2label, label2id=label2id)\n","model = YolosForObjectDetection.from_pretrained(MODEL_CHECKPOINT, config=config,ignore_mismatched_sizes= True)\n","\n","# Log model architecture summary to wandb\n","wandb.config.update({\n","    \"model_params\": sum(p.numel() for p in model.parameters()),\n","    \"trainable_params\": sum(p.numel() for p in model.parameters() if p.requires_grad),\n","})\n","\n","# Enable gradient checkpointing to save memory\n","model.gradient_checkpointing_enable()\n","\n","# Define training arguments\n","training_args = TrainingArguments(\n","    output_dir=OUTPUT_DIR,\n","    per_device_train_batch_size=BATCH_SIZE,\n","    per_device_eval_batch_size=BATCH_SIZE,\n","    learning_rate=LEARNING_RATE,\n","    num_train_epochs=NUM_EPOCHS,\n","    warmup_ratio=WARMUP_RATIO,\n","    weight_decay=0.01,\n","    eval_strategy=\"steps\",\n","    eval_steps=EVAL_STEPS,\n","    save_strategy=\"steps\",\n","    save_steps=SAVE_STEPS,\n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"eval_loss\",\n","    greater_is_better=False,\n","    push_to_hub=False,\n","    logging_dir=os.path.join(OUTPUT_DIR, \"logs\"),\n","    logging_steps=100,\n","    fp16=True,\n","    gradient_accumulation_steps=GRAD_ACCUMULATION_STEPS,\n","    save_total_limit=3,\n","    dataloader_num_workers=NUM_WORKERS,\n","    dataloader_drop_last=True,\n","    dataloader_pin_memory=True,\n","    report_to=\"wandb\",\n","    gradient_checkpointing=True,\n","    ddp_find_unused_parameters=False,\n",")\n","\n","# Initialize Trainer with early stopping and wandb\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset,\n","    data_collator=custom_collate_fn,\n","    callbacks=[\n","        EarlyStoppingCallback(early_stopping_patience=EARLY_STOPPING_PATIENCE),\n","    ],\n",")\n","\n","# Train the model\n","print(\"Starting training...\")\n","train_result = trainer.train()\n","\n","# Save the final model\n","final_model_path = os.path.join(OUTPUT_DIR, \"final_model\")\n","trainer.save_model(final_model_path)\n","print(f\"Model saved to {final_model_path}\")\n","\n","# Create a validation dataset subset for final evaluation\n","print(\"Performing final evaluation...\")\n","eval_subset_size = min(1000, len(val_dataset))\n","eval_subset_indices = torch.randperm(len(val_dataset))[:eval_subset_size]\n","eval_subset = torch.utils.data.Subset(val_dataset, eval_subset_indices)\n","\n","# Evaluate the model on the validation subset\n","eval_results = trainer.evaluate(eval_dataset=eval_subset)\n","print(f\"Final evaluation results: {eval_results}\")\n","\n","\n","# Finish wandb run\n","wandb.finish()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":492},"id":"EUupvDbv5VYN","executionInfo":{"status":"error","timestamp":1745003801702,"user_tz":-180,"elapsed":78680,"user":{"displayName":"Nisan","userId":"17072699878303939405"}},"outputId":"c291dcd7-648f-4e3d-f1e7-16f440124f0c"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading YOLOS model from hustvl/yolos-small\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of YolosForObjectDetection were not initialized from the model checkpoint at hustvl/yolos-small and are newly initialized because the shapes did not match:\n","- class_labels_classifier.layers.2.bias: found shape torch.Size([92]) in the checkpoint and torch.Size([9]) in the model instantiated\n","- class_labels_classifier.layers.2.weight: found shape torch.Size([92, 384]) in the checkpoint and torch.Size([9, 384]) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Starting training...\n"]},{"output_type":"error","ename":"TypeError","evalue":"list indices must be integers or slices, not str","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-25-df7924a1b5f2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting training...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m \u001b[0mtrain_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;31m# Save the final model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2243\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2244\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2245\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2246\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2247\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2558\u001b[0m                     )\n\u001b[1;32m   2559\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2560\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2562\u001b[0m                     if (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3735\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3736\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3738\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3799\u001b[0m                 \u001b[0mloss_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_items_in_batch\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3800\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mloss_kwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3801\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3802\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3803\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/yolos/modeling_yolos.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pixel_values, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    842\u001b[0m                 \u001b[0moutputs_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_labels_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m                 \u001b[0moutputs_coord\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbbox_predictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 844\u001b[0;31m             loss, loss_dict, auxiliary_outputs = self.loss_function(\n\u001b[0m\u001b[1;32m    845\u001b[0m                 \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_boxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs_coord\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/loss/loss_for_object_detection.py\u001b[0m in \u001b[0;36mForObjectDetectionLoss\u001b[0;34m(logits, labels, device, pred_boxes, config, outputs_class, outputs_coord, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m         \u001b[0moutputs_loss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"auxiliary_outputs\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mauxiliary_outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m     \u001b[0mloss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m     \u001b[0;31m# Fourth: compute total loss, as a weighted sum of the various losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m     \u001b[0mweight_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"loss_ce\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"loss_bbox\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbbox_loss_coefficient\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/loss/loss_for_object_detection.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, outputs, targets)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;31m# Retrieve the matching between the outputs of the last layer and the targets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs_without_aux\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0;31m# Compute the average number of target boxes across all nodes, for normalization purposes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/loss/loss_for_object_detection.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, outputs, targets)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;31m# Also concat the target labels and boxes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m         \u001b[0mtarget_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"class_labels\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m         \u001b[0mtarget_bbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"boxes\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/loss/loss_for_object_detection.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;31m# Also concat the target labels and boxes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m         \u001b[0mtarget_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"class_labels\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m         \u001b[0mtarget_bbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"boxes\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"]}]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMuc4ow1pJXJH1kskuj25GA"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}